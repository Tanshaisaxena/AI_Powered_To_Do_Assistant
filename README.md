# ğŸ§  RAG-powered To-Do Assistant (Go + Gin + Ollama)

A Retrieval-Augmented Generation (RAG) based To-Do Assistant built in Go using Gin and Ollama.
This project evolves step-by-step from a basic TODO API into a fully functional semantic AI assistant.

---

## ğŸš€ Features

- CRUD operations for tasks
- Keyword-based search (baseline)
- Semantic search using embeddings
- Local embeddings with Ollama (no API keys required)
- Cosine similarity for relevance ranking
- Top-K and similarity threshold filtering
- LLM-powered natural language answers
- Hallucination-safe prompt design
- Fully local RAG pipeline

---

## ğŸ—ï¸ Architecture Overview

User Question
  â†’ Question Embedding
  â†’ Similarity Search (Task Embeddings)
  â†’ Top-K + Threshold Filtering
  â†’ Prompt Construction
  â†’ LLM Generation (Ollama)
  â†’ Final Answer

---

## ğŸ§© Tech Stack

- Language: Go
- Framework: Gin
- Embeddings: Ollama (nomic-embed-text)
- LLM: Ollama (mistral / llama3)
- Similarity: Cosine Similarity
- Storage: In-memory (learning phase)

---

## ğŸ“ Project Structure

handlers/    â†’ API handlers (CRUD, AskRAG)
helpers/     â†’ Embeddings, similarity, prompt & LLM helpers
models/      â†’ Data models
constants/   â†’ Log constants
main.go      â†’ App entry point

---

## ğŸ”§ Setup Instructions

### Prerequisites
- Go 1.21+
- Ollama installed

### Install models
ollama pull nomic-embed-text
ollama pull mistral

### Run
go run main.go

Server runs on http://localhost:8080

---

## ğŸ“Œ API Endpoints

POST /task        â†’ Create task (embedding generated automatically)
GET /task         â†’ List tasks
DELETE /task/:id  â†’ Delete task
POST /ask         â†’ Ask questions using token count
POST /ask-rag     â†’ Ask questions using RAG

---

## ğŸ§  RAG Flow

1. Task creation â†’ embedding stored
2. User question â†’ embedding generated
3. Similarity computed against all tasks
4. Results sorted by relevance
5. Threshold + Top-K applied
6. Prompt built with grounded context
7. LLM generates final answer

---

## ğŸ” Hallucination Safety

The LLM is restricted to answering only from retrieved tasks.
If information is missing, it responds with "I don't know".

---

## ğŸ› ï¸ Future Enhancements

- pgvector + Postgres
- Streaming responses
- Citations per task
- Conversation memory
- UI frontend

---

## ğŸ† Key Takeaway

This project demonstrates how real-world RAG systems are built:
retrieval first, generation second.

---

## ğŸ“œ License

MIT
